{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd5c0cf6-4b96-4c12-a967-87884599340a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbc0aad8-1985-4fd0-bc65-20bafca79237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "PATH_COLLECTION_DATA = 'https://gitlab.com/checkthat_lab/clef2025-checkthat-lab/-/raw/701a0a217286555445870e1005d637ff587c5cee/task4/subtask_4b/subtask4b_collection_data.pkl'\n",
    "PATH_QUERY_TRAIN_DATA = 'https://gitlab.com/checkthat_lab/clef2025-checkthat-lab/-/raw/main/task4/subtask_4b/subtask4b_query_tweets_train.tsv?inline=false'\n",
    "PATH_QUERY_DEV_DATA = 'https://gitlab.com/checkthat_lab/clef2025-checkthat-lab/-/raw/main/task4/subtask_4b/subtask4b_query_tweets_dev.tsv?inline=false'\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "df_collection = pd.read_pickle(PATH_COLLECTION_DATA)\n",
    "df_query_train = pd.read_csv(PATH_QUERY_TRAIN_DATA, sep='\\t')\n",
    "df_query_dev = pd.read_csv(PATH_QUERY_DEV_DATA, sep='\\t')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "151273bc-c12d-45fd-a669-cb65f5cfbc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding wrapper\n",
    "class EmbeddingWrapper:\n",
    "    def __init__(self, text_list, model_name, device):\n",
    "        self.text_list = text_list\n",
    "        self.model_name = model_name\n",
    "        self.device = device\n",
    "\n",
    "    def calculate_embeddings(self, batch_size=32):\n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        model = AutoModel.from_pretrained(self.model_name).to(self.device)\n",
    "        model.eval()\n",
    "\n",
    "        embeddings = []\n",
    "        with torch.no_grad():\n",
    "            for i in tqdm(range(0, len(self.text_list), batch_size), desc=f\"Encoding with {self.model_name}\"):\n",
    "                batch_texts = self.text_list[i:i + batch_size]\n",
    "                inputs = tokenizer(batch_texts, padding=True, truncation=True, return_tensors=\"pt\",\n",
    "                                   return_token_type_ids=False, max_length=512)\n",
    "                inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "                outputs = model(**inputs)\n",
    "                batch_embeddings = outputs.last_hidden_state[:, 0, :]  # [CLS]\n",
    "                embeddings.append(batch_embeddings.cpu())\n",
    "        self.embeddings = torch.cat(embeddings, dim=0)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5146736a-e721-4b34-a5a3-b9a4d4a23147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Evaluating model: allenai/specter2_base ==========\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'get_default_device'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 48\u001b[0m\n\u001b[1;32m     45\u001b[0m text_query_dev \u001b[38;5;241m=\u001b[39m df_query_dev[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtweet_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Embeddings\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m emb_collection \u001b[38;5;241m=\u001b[39m \u001b[43mEmbeddingWrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_collection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m emb_query_train \u001b[38;5;241m=\u001b[39m EmbeddingWrapper(text_query_train, model_name\u001b[38;5;241m=\u001b[39mmodel_name, device\u001b[38;5;241m=\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mcalculate_embeddings()\n\u001b[1;32m     50\u001b[0m emb_query_dev \u001b[38;5;241m=\u001b[39m EmbeddingWrapper(text_query_dev, model_name\u001b[38;5;241m=\u001b[39mmodel_name, device\u001b[38;5;241m=\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mcalculate_embeddings()\n",
      "Cell \u001b[0;32mIn[8], line 10\u001b[0m, in \u001b[0;36mEmbeddingWrapper.calculate_embeddings\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_embeddings\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m):\n\u001b[1;32m      9\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name)\n\u001b[0;32m---> 10\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     11\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     13\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:571\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mconfig_class \u001b[38;5;241m==\u001b[39m config\u001b[38;5;241m.\u001b[39msub_configs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    570\u001b[0m         config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget_text_config()\n\u001b[0;32m--> 571\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    576\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    577\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/modeling_utils.py:309\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    307\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    311\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/modeling_utils.py:4252\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4250\u001b[0m \u001b[38;5;66;03m# Potentially detect context manager or global device, and use it (only if no device_map was provided)\u001b[39;00m\n\u001b[1;32m   4251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_deepspeed_zero3_enabled():\n\u001b[0;32m-> 4252\u001b[0m     device_in_context \u001b[38;5;241m=\u001b[39m \u001b[43mget_torch_context_manager_or_global_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4253\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m device_in_context \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   4254\u001b[0m         \u001b[38;5;66;03m# TODO Cyril: raise an error instead of the warning in v4.53 (and change the test to check for raise instead of success)\u001b[39;00m\n\u001b[1;32m   4255\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m   4256\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWe detected that you are using `from_pretrained` with a meta device context manager or `torch.set_default_device(\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)`\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4257\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is an anti-pattern and will raise an Error in version v4.53\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf you want to initialize a model on the meta device, use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4258\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe context manager or global device with `from_config`, or `ModelClass(config)`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4259\u001b[0m         )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/modeling_utils.py:322\u001b[0m, in \u001b[0;36mget_torch_context_manager_or_global_device\u001b[0;34m()\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;124;03mTest if a device context manager is currently in use, or if it is not the case, check if the default device\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;124;03mis not \"cpu\". This is used to infer the correct device to load the model on, in case `device_map` is not provided.\u001b[39;00m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    321\u001b[0m device_in_context \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([])\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m--> 322\u001b[0m default_device \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_default_device\u001b[49m()\n\u001b[1;32m    323\u001b[0m \u001b[38;5;66;03m# This case means no context manager was used -> we still check if the default that was potentially set is not cpu\u001b[39;00m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device_in_context \u001b[38;5;241m==\u001b[39m default_device:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/__init__.py:1833\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m   1830\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimportlib\u001b[39;00m\n\u001b[1;32m   1831\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m-> 1833\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch' has no attribute 'get_default_device'"
     ]
    }
   ],
   "source": [
    "# Top-k retrieval\n",
    "def get_top_k_cords(emb_query, emb_collection, df_collection, k=30):\n",
    "    cos_sim_matrix = cosine_similarity(emb_query, emb_collection)\n",
    "    top_k_indices = np.argsort(-cos_sim_matrix, axis=1)[:, :k]\n",
    "    top_k_cord_uids = df_collection.iloc[top_k_indices.flatten()]['cord_uid'].values.reshape(top_k_indices.shape)\n",
    "    return top_k_cord_uids.tolist()\n",
    "\n",
    "# MRR\n",
    "def get_performance_mrr(data, col_gold, col_pred, list_k=[1, 5, 10]):\n",
    "    d_performance = {}\n",
    "    for k in list_k:\n",
    "        data[\"in_topx\"] = data.apply(lambda x: (1 / ([i for i in x[col_pred][:k]].index(x[col_gold]) + 1)\n",
    "                                                if x[col_gold] in [i for i in x[col_pred][:k]] else 0), axis=1)\n",
    "        d_performance[k] = data[\"in_topx\"].mean()\n",
    "    return d_performance\n",
    "\n",
    "# Models to evaluate\n",
    "models_to_test = [\n",
    "    'allenai/specter2_base',\n",
    "    'sentence-transformers/all-MiniLM-L6-v2',\n",
    "    'bert-base-uncased',\n",
    "    'allenai/scibert_scivocab_uncased',\n",
    "    'sentence-transformers/msmarco-distilbert-base-v4',\n",
    "    'intfloat/e5-base-v2',\n",
    "    'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract',\n",
    "    'sentence-transformers/all-mpnet-base-v2',\n",
    "    'jinaai/jina-embeddings-v2-base-en',\n",
    "    'nlpaueb/legal-bert-base-uncased',\n",
    "    'sentence-transformers/paraphrase-multilingual-mpnet-base-v2',\n",
    "    'sentence-transformers/paraphrase-TinyBERT-L6-v2',\n",
    "    'mixedbread-ai/mxbai-embed-large-v1'\n",
    "    \n",
    "]\n",
    "\n",
    "# Evaluation\n",
    "all_results = {}\n",
    "\n",
    "for model_name in models_to_test:\n",
    "    print(f\"\\n========== Evaluating model: {model_name} ==========\")\n",
    "\n",
    "    # Prepare text input\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    text_collection = [title + tokenizer.sep_token + abstract for title, abstract in zip(df_collection['title'], df_collection['abstract'])]\n",
    "    text_query_train = df_query_train['tweet_text'].tolist()\n",
    "    text_query_dev = df_query_dev['tweet_text'].tolist()\n",
    "\n",
    "    # Embeddings\n",
    "    emb_collection = EmbeddingWrapper(text_collection, model_name=model_name, device=device).calculate_embeddings()\n",
    "    emb_query_train = EmbeddingWrapper(text_query_train, model_name=model_name, device=device).calculate_embeddings()\n",
    "    emb_query_dev = EmbeddingWrapper(text_query_dev, model_name=model_name, device=device).calculate_embeddings()\n",
    "\n",
    "    # Retrieval\n",
    "    df_query_train[f'topk_{model_name}'] = get_top_k_cords(emb_query_train.embeddings, emb_collection.embeddings, df_collection)\n",
    "    df_query_dev[f'topk_{model_name}'] = get_top_k_cords(emb_query_dev.embeddings, emb_collection.embeddings, df_collection)\n",
    "\n",
    "    # Evaluation\n",
    "    results_train = get_performance_mrr(df_query_train, 'cord_uid', f'topk_{model_name}')\n",
    "    results_dev = get_performance_mrr(df_query_dev, 'cord_uid', f'topk_{model_name}')\n",
    "\n",
    "    all_results[model_name] = {\n",
    "        'train': results_train,\n",
    "        'dev': results_dev\n",
    "    }\n",
    "\n",
    "    # Print scores\n",
    "    print(f\"Train MRR: {results_train}\")\n",
    "    print(f\"Dev MRR:   {results_dev}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18935e4a-a4b4-446f-83dd-dafa5cf8e7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"\\n======= Summary of All Models =======\")\n",
    "for model, results in all_results.items():\n",
    "    print(f\"\\nModel: {model}\")\n",
    "    print(f\"Train MRR: {results['train']}\")\n",
    "    print(f\"Dev MRR:   {results['dev']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689578e7-1ad8-4d73-8d44-24ea0d5d92f5",
   "metadata": {},
   "source": [
    "All runs:\n",
    "\n",
    "Model: allenai/specter2_base\n",
    "Train MRR: {1: 0.403096553333852, 5: 0.47030135636297626, 10: 0.47891934932120095}\n",
    "Dev MRR:   {1: 0.4357142857142857, 5: 0.49826190476190474, 10: 0.5065691609977324}\n",
    "\n",
    "Model: sentence-transformers/all-MiniLM-L6-v2\n",
    "\n",
    "Train MRR: {1: 0.3298840737570995, 5: 0.3967322804014627, 10: 0.40613950915047936}\n",
    "Dev MRR:   {1: 0.3435714285714286, 5: 0.4059642857142857, 10: 0.4150019841269841}\n",
    "\n",
    "Model: bert-base-uncased\n",
    "Train MRR: {1: 0.0071578619777483855, 5: 0.01069140797219845, 10: 0.011623529310061637}\n",
    "Dev MRR:   {1: 0.013571428571428571, 5: 0.01776190476190476, 10: 0.018869614512471655}\n",
    "\n",
    "Model: allenai/scibert_scivocab_uncased\n",
    "Train MRR: {1: 0.004823776550221738, 5: 0.008070748722736585, 10: 0.00893553354846438}\n",
    "Dev MRR:   {1: 0.007857142857142858, 5: 0.009488095238095236, 10: 0.009815476190476189}\n",
    "\n",
    "Model: sentence-transformers/msmarco-distilbert-base-v4\n",
    "Train MRR: {1: 0.3257605228351358, 5: 0.38241785316009236, 10: 0.39129639303528674}\n",
    "Dev MRR:   {1: 0.32357142857142857, 5: 0.37922619047619044, 10: 0.38698724489795916}\n",
    "\n",
    "Model: intfloat/e5-base-v2\n",
    "Train MRR: {1: 0.07826966466972692, 5: 0.10394590108664643, 10: 0.1088433124747604}\n",
    "Dev MRR:   {1: 0.1, 5: 0.12107142857142857, 10: 0.1259985827664399}\n",
    "\n",
    "Model: microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\n",
    "Train MRR: {1: 0.008402707539095932, 5: 0.010779584532793901, 10: 0.011319882085462106}\n",
    "Dev MRR:   {1: 0.01, 5: 0.012773809523809524, 10: 0.012924603174603172}\n",
    "\n",
    "Model: sentence-transformers/all-mpnet-base-v2\n",
    "Train MRR: {1: 0.3861355325604917, 5: 0.4509790191654348, 10: 0.4602069617494032}\n",
    "Dev MRR:   {1: 0.3964285714285714, 5: 0.4585000000000001, 10: 0.46760289115646253}\n",
    "\n",
    "Model: jinaai/jina-embeddings-v2-base-en\n",
    "Train MRR: {1: 0.00023340854275266475, 5: 0.00048108094089577015, 10: 0.0006447139139895695}\n",
    "Dev MRR:   {1: 0.0, 5: 0.00014285714285714287, 10: 0.00014285714285714287}\n",
    "\n",
    "Model: nlpaueb/legal-bert-base-uncased\n",
    "Train MRR: {1: 0.0036567338364584144, 5: 0.005326901631266371, 10: 0.0057552495310217245}\n",
    "Dev MRR:   {1: 0.0064285714285714285, 5: 0.006904761904761904, 10: 0.00709608843537415}\n",
    "\n",
    "Model: sentence-transformers/paraphrase-multilingual-mpnet-base-v2\n",
    "Train MRR: {1: 0.1528047926554112, 5: 0.19350086879846468, 10: 0.2001792861650482}\n",
    "Dev MRR:   {1: 0.16, 5: 0.20447619047619048, 10: 0.2115909863945578}\n",
    "\n",
    "Model: sentence-transformers/paraphrase-TinyBERT-L6-v2\n",
    "Train MRR: {1: 0.18135843771882051, 5: 0.22694571954666876, 10: 0.23494391402661846}\n",
    "Dev MRR:   {1: 0.19857142857142857, 5: 0.24644047619047618, 10: 0.25355243764172336}\n",
    "\n",
    "Model: mixedbread-ai/mxbai-embed-large-v1\n",
    "Train MRR: {1: 0.529837392048549, 5: 0.5973222853289764, 10: 0.6045375732180369}\n",
    "Dev MRR:   {1: 0.5471428571428572, 5: 0.6070238095238095, 10: 0.6137389455782313}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9b19e0-2068-4241-a841-f515450addb8",
   "metadata": {},
   "source": [
    "So the model performing the best was the:\n",
    "\n",
    "mixedbread-ai/mxbai-embed-large-v1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
