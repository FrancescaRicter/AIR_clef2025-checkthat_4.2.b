{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e7f1968-07cd-4181-b32d-f02f61e98a9c",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9df5bb7a-3052-41c5-9c76-04a25ab63121",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_def.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_avx2.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_core.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_intel_lp64.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_intel_thread.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_def.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_avx2.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_core.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_intel_lp64.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_intel_thread.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "Collecting rank_bm25\n",
      "  Using cached rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from rank_bm25) (1.26.4)\n",
      "Using cached rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
      "Installing collected packages: rank_bm25\n",
      "Successfully installed rank_bm25-0.2.2\n",
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_def.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_avx2.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_core.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_intel_lp64.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_intel_thread.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_def.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_avx2.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_core.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_intel_lp64.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_intel_thread.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "Collecting sentence-transformers\n",
      "  Using cached sentence_transformers-4.1.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.11/site-packages (4.45.1)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from sentence-transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.11/site-packages (from sentence-transformers) (2.1.2+cu121)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.11/site-packages (from sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.11/site-packages (from sentence-transformers) (1.15.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /opt/conda/lib/python3.11/site-packages (from sentence-transformers) (0.30.2)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.11/site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /opt/conda/lib/python3.11/site-packages (from sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.4.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: triton==2.1.0 in /opt/conda/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (2.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Using cached sentence_transformers-4.1.0-py3-none-any.whl (345 kB)\n",
      "Installing collected packages: sentence-transformers\n",
      "Successfully installed sentence-transformers-4.1.0\n",
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_def.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_avx2.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_core.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_intel_lp64.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_intel_thread.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_def.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_avx2.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_core.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_intel_lp64.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_intel_thread.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "Collecting datasets\n",
      "  Using cached datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (17.0.0)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.11/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.11/site-packages (from datasets) (4.66.5)\n",
      "Collecting xxhash (from datasets)\n",
      "  Using cached xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Using cached multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (0.30.2)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.11/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10.10)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.14.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.11/site-packages (from yarl<2.0,>=1.12.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.2.0)\n",
      "Using cached datasets-3.6.0-py3-none-any.whl (491 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "Using cached multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "Using cached xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Installing collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.3.2\n",
      "    Uninstalling fsspec-2025.3.2:\n",
      "      Successfully uninstalled fsspec-2025.3.2\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.9\n",
      "    Uninstalling dill-0.3.9:\n",
      "      Successfully uninstalled dill-0.3.9\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed datasets-3.6.0 dill-0.3.8 fsspec-2025.3.0 multiprocess-0.70.16 xxhash-3.5.0\n",
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_def.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_avx2.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_core.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_intel_lp64.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_intel_thread.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_def.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_avx2.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_core.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_intel_lp64.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_intel_thread.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "Collecting accelerate\n",
      "  Using cached accelerate-1.7.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /opt/conda/lib/python3.11/site-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from accelerate) (24.2)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.11/site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from accelerate) (2.1.2+cu121)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /opt/conda/lib/python3.11/site-packages (from accelerate) (0.30.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.11/site-packages (from accelerate) (0.5.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (2025.3.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (3.4.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: triton==2.1.0 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (2.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.1.31)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Using cached accelerate-1.7.0-py3-none-any.whl (362 kB)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-1.7.0\n",
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_def.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_avx2.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_core.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_intel_lp64.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_intel_thread.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_def.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_avx2.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_core.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_intel_lp64.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_intel_thread.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.11/site-packages (4.45.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.11/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "!pip install rank_bm25\n",
    "!pip install sentence-transformers transformers\n",
    "!pip install datasets\n",
    "!pip install accelerate\n",
    "!pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c16f68f-5c4f-454e-9e8b-eb5e9686a856",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"USE_TF\"] = \"0\" # disable TensorFlow to avoid problems later\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from datasets import Dataset\n",
    "from sentence_transformers import InputExample, losses, SentenceTransformer, models\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826eaa15-05fd-40dc-8b44-41e9e9ed46e5",
   "metadata": {},
   "source": [
    "# Loading Data\n",
    "We load the data from the gitlab repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37abb6ec-cf98-49c8-b36b-398f9f70f8bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH_COLLECTION_DATA = 'https://gitlab.com/checkthat_lab/clef2025-checkthat-lab/-/raw/701a0a217286555445870e1005d637ff587c5cee/task4/subtask_4b/subtask4b_collection_data.pkl'\n",
    "PATH_QUERY_TRAIN_DATA = 'https://gitlab.com/checkthat_lab/clef2025-checkthat-lab/-/raw/main/task4/subtask_4b/subtask4b_query_tweets_train.tsv?inline=false'\n",
    "PATH_QUERY_DEV_DATA = 'https://gitlab.com/checkthat_lab/clef2025-checkthat-lab/-/raw/main/task4/subtask_4b/subtask4b_query_tweets_dev.tsv?inline=false'\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "df_collection = pd.read_pickle(PATH_COLLECTION_DATA)\n",
    "df_query_train = pd.read_csv(PATH_QUERY_TRAIN_DATA, sep = '\\t')\n",
    "df_query_dev = pd.read_csv(PATH_QUERY_DEV_DATA, sep = '\\t')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bd7a67-2fc9-4414-b9dd-925098f57cea",
   "metadata": {},
   "source": [
    "## Baseline \"neural\" NLP representation learning approach  \n",
    "### (MXBAI Embed Large v1 without any finetuning or preprocessing)  \n",
    "As a baseline for this approach we used **MXBAI Embed Large v1**. It is a transformer-based model trained on diverse open-domain text pairs to produce high-quality sentence embeddings. Without any task-specific finetuning, it serves as a strong general-purpose baseline for linking tweets to academic papers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a102e75d-3dc9-4b3e-9b6e-f8d523b9aa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingWrapper:\n",
    "    \n",
    "    def __init__(self, text_list, model_name, device):\n",
    "        self.text_list = text_list\n",
    "        self.model_name = model_name\n",
    "        self.device = device\n",
    "        \n",
    "    def calculate_embeddings(self, batch_size = 32):\n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        model = AutoModel.from_pretrained(self.model_name).to(self.device)\n",
    "        model.eval()\n",
    "    \n",
    "        embeddings = []\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            for i in tqdm(range(0, len(self.text_list), batch_size), desc=\"Encoding\"):\n",
    "                batch_texts = self.text_list[i:i+batch_size]\n",
    "                inputs = tokenizer(batch_texts, padding=True, truncation=True, return_tensors=\"pt\",\n",
    "                                   return_token_type_ids=False, max_length=512)\n",
    "                inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "                outputs = model(**inputs)\n",
    "                batch_embeddings = outputs.last_hidden_state[:, 0, :] \n",
    "                embeddings.append(batch_embeddings.cpu())\n",
    "    \n",
    "        self.embeddings = torch.cat(embeddings, dim=0)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa88471e-6d9e-4753-a3b2-71307b1ea83d",
   "metadata": {},
   "source": [
    "### Document Embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "136ceac8-f4f3-4940-ade1-0f643cff161c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding:   2%|▏         | 5/242 [00:16<12:51,  3.25s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m  AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmixedbread-ai/mxbai-embed-large-v1\u001b[39m\u001b[38;5;124m'\u001b[39m)       \n\u001b[1;32m      2\u001b[0m text_batch \u001b[38;5;241m=\u001b[39m [title \u001b[38;5;241m+\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39msep_token \u001b[38;5;241m+\u001b[39m abstract \u001b[38;5;28;01mfor\u001b[39;00m title, abstract \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(df_collection[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m], df_collection[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mabstract\u001b[39m\u001b[38;5;124m'\u001b[39m])]\n\u001b[0;32m----> 3\u001b[0m emb_collection \u001b[38;5;241m=\u001b[39m \u001b[43mEmbeddingWrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmixedbread-ai/mxbai-embed-large-v1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 23\u001b[0m, in \u001b[0;36mEmbeddingWrapper.calculate_embeddings\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     21\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[1;32m     22\u001b[0m         batch_embeddings \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state[:, \u001b[38;5;241m0\u001b[39m, :] \n\u001b[0;32m---> 23\u001b[0m         embeddings\u001b[38;5;241m.\u001b[39mappend(\u001b[43mbatch_embeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(embeddings, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tokenizer =  AutoTokenizer.from_pretrained('mixedbread-ai/mxbai-embed-large-v1')       \n",
    "text_batch = [title + tokenizer.sep_token + abstract for title, abstract in zip(df_collection['title'], df_collection['abstract'])]\n",
    "emb_collection = EmbeddingWrapper(text_batch,model_name='mixedbread-ai/mxbai-embed-large-v1', device=device).calculate_embeddings(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ec07e6-f9e8-475e-8ed5-188be26f356b",
   "metadata": {},
   "source": [
    "### Tweet Embeddings:\n",
    "We only look at tweet text and keep the order so the ground truth can be matched later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8df625-948b-47df-a92a-9cd6c22197c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_query_train = df_query_train['tweet_text'].to_list()\n",
    "emb_query_train = EmbeddingWrapper(text_query_train,model_name='mixedbread-ai/mxbai-embed-large-v1', device=device).calculate_embeddings(32)\n",
    "\n",
    "text_query_dev = df_query_dev['tweet_text'].to_list()\n",
    "emb_query_dev = EmbeddingWrapper(text_query_dev,model_name='mixedbread-ai/mxbai-embed-large-v1', device=device).calculate_embeddings(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe9d990-561e-48b3-a80b-d4be53eca1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_query_dev[0:2])\n",
    "print(emb_query_dev.embeddings[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec8738fb-be84-44ef-a4c8-a38d9235cea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_performance_mrr(data, col_gold, col_pred, list_k = [1, 5, 10]):\n",
    "    d_performance = {}\n",
    "    for k in list_k:\n",
    "        data[\"in_topx\"] = data.apply(lambda x: (1/([i for i in x[col_pred][:k]].index(x[col_gold]) + 1) if x[col_gold] in [i for i in x[col_pred][:k]] else 0), axis=1)\n",
    "        #performances.append(data[\"in_topx\"].mean())\n",
    "        d_performance[k] = data[\"in_topx\"].mean()\n",
    "    return d_performance\n",
    "    \n",
    "def get_top_k_cords(emb_query, emb_collection, df_collection, k=30):\n",
    "    # compute cosine similarity matrix (for each query consine similarity for each document)\n",
    "    cos_sim_matrix = cosine_similarity(emb_query, emb_collection)\n",
    "    # For each query, get the indices of the top-k documents\n",
    "    top_k_indices = np.argsort(-cos_sim_matrix, axis=1)[:, :k]  # shape: (num_queries, k)\n",
    "\n",
    "    top_k_cord_uids = df_collection.iloc[top_k_indices.flatten()]['cord_uid'].values.reshape(top_k_indices.shape) # shape: (num_queries_topcords)\n",
    "    return top_k_cord_uids.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9d0d24-b997-4609-9bbd-781fe9caf75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_query_train['topk'] = get_top_k_cords(emb_query_train.embeddings, emb_collection.embeddings, df_collection)\n",
    "df_query_dev['topk'] = get_top_k_cords(emb_query_dev.embeddings, emb_collection.embeddings, df_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f956fa12-f867-4db8-ab69-78a039e0d912",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_train = get_performance_mrr(df_query_train, 'cord_uid', 'topk')\n",
    "results_dev = get_performance_mrr(df_query_dev, 'cord_uid', 'topk')\n",
    "\n",
    "print(f\"Results on the train set: {results_train}\")\n",
    "print(f\"Results on the dev set: {results_dev}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd46042b-cdda-4a93-80df-b9d1c7724391",
   "metadata": {},
   "source": [
    "As we can see the model already performs better than the baseline but we would still like to improve the model by doing some finetuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b349a8-9bb0-49cd-8508-80b21d6d5c7e",
   "metadata": {},
   "source": [
    "## fine-tuning MXBAI Embed Large v1\n",
    "\n",
    "To make our model perform better we are fine-tuning the model. We use hard positives (like a tweet and the matching paper) and hard negatives (a tweet and a similar paper (bm25) which is not the original paper) to train our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5227e8-8548-44e7-9a1c-c3669f38644f",
   "metadata": {},
   "source": [
    "### Generating training pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6944f9-773d-43eb-a48e-7f58d533a335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query + title + label\n",
    "query_texts = df_query_train['tweet_text'].tolist()\n",
    "true_uids = df_query_train['cord_uid'].tolist()\n",
    "collection_texts = df_collection['title'].fillna('').tolist()\n",
    "collection_uids = df_collection['cord_uid'].tolist()\n",
    "\n",
    "# Tokenize and build index\n",
    "tokenized_corpus = [doc.split() for doc in collection_texts]\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "positive_pairs = []\n",
    "negative_pairs = []\n",
    "\n",
    "for qtext, true_uid in zip(query_texts, true_uids):\n",
    "    # Positive\n",
    "    try:\n",
    "        pos_idx = collection_uids.index(true_uid)\n",
    "        pos_doc = collection_texts[pos_idx]\n",
    "        positive_pairs.append((qtext, pos_doc, 1))\n",
    "    except ValueError:\n",
    "        continue\n",
    "\n",
    "    # Hard Negatives from BM25\n",
    "    scores = bm25.get_scores(qtext.split())\n",
    "    top_indices = np.argsort(scores)[::-1]\n",
    "    negs = 0\n",
    "    for idx in top_indices:\n",
    "        if collection_uids[idx] != true_uid:\n",
    "            negative_pairs.append((qtext, collection_texts[idx], 0))\n",
    "            negs += 1\n",
    "        if negs == 1:\n",
    "            break\n",
    "\n",
    "df_query_title_triples = pd.DataFrame(positive_pairs + negative_pairs, columns=[\"query\", \"document\", \"label\"])\n",
    "df_query_title_triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410c60a7-49cf-49ae-bbe7-d33be6cdc02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_query_title_triples.to_csv(\"df_query_title_triples.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c3fb00b-79ce-4635-b3fa-83af379776c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Oral care in rehabilitation medicine: oral vul...</td>\n",
       "      <td>Oral Management in Rehabilitation Medicine: Or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>this study isn't receiving sufficient attentio...</td>\n",
       "      <td>Variation in racial/ethnic disparities in COVI...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>thanks, xi jinping. a reminder that this study...</td>\n",
       "      <td>Effect of non-pharmaceutical interventions for...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Taiwan - a population of 23 million has had ju...</td>\n",
       "      <td>Potential lessons from the Taiwan and New Zeal...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Obtaining a diagnosis of autism in lower incom...</td>\n",
       "      <td>Autism screening and conditional cash transfer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25701</th>\n",
       "      <td>\"evidence on covid-19 reveals a growing body o...</td>\n",
       "      <td>Airborne transmission of SARS-CoV-2 over dista...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25702</th>\n",
       "      <td>Outdoor lighting has detrimental impacts on lo...</td>\n",
       "      <td>Artificial nighttime lighting impacts visual e...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25703</th>\n",
       "      <td>26/ and influenza virus (and other pathogens, ...</td>\n",
       "      <td>Emerging Pandemic Diseases: How We Got To COVI...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25704</th>\n",
       "      <td>does it?'sars-cov-2-naïve vaccinees had a 13.0...</td>\n",
       "      <td>SARS-CoV-2 Naturally Acquired Immunity vs. Vac...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25705</th>\n",
       "      <td>when \"the airway immune cells of children are ...</td>\n",
       "      <td>Direct exposure to SARS-CoV-2 and cigarette sm...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25706 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   query  \\\n",
       "0      Oral care in rehabilitation medicine: oral vul...   \n",
       "1      this study isn't receiving sufficient attentio...   \n",
       "2      thanks, xi jinping. a reminder that this study...   \n",
       "3      Taiwan - a population of 23 million has had ju...   \n",
       "4      Obtaining a diagnosis of autism in lower incom...   \n",
       "...                                                  ...   \n",
       "25701  \"evidence on covid-19 reveals a growing body o...   \n",
       "25702  Outdoor lighting has detrimental impacts on lo...   \n",
       "25703  26/ and influenza virus (and other pathogens, ...   \n",
       "25704  does it?'sars-cov-2-naïve vaccinees had a 13.0...   \n",
       "25705  when \"the airway immune cells of children are ...   \n",
       "\n",
       "                                                document  label  \n",
       "0      Oral Management in Rehabilitation Medicine: Or...      1  \n",
       "1      Variation in racial/ethnic disparities in COVI...      1  \n",
       "2      Effect of non-pharmaceutical interventions for...      1  \n",
       "3      Potential lessons from the Taiwan and New Zeal...      1  \n",
       "4      Autism screening and conditional cash transfer...      1  \n",
       "...                                                  ...    ...  \n",
       "25701  Airborne transmission of SARS-CoV-2 over dista...      0  \n",
       "25702  Artificial nighttime lighting impacts visual e...      0  \n",
       "25703  Emerging Pandemic Diseases: How We Got To COVI...      0  \n",
       "25704  SARS-CoV-2 Naturally Acquired Immunity vs. Vac...      0  \n",
       "25705  Direct exposure to SARS-CoV-2 and cigarette sm...      0  \n",
       "\n",
       "[25706 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# query + title and abstract + label\n",
    "collection_texts = (df_collection['title'].fillna('') + ' ' + df_collection['abstract'].fillna('')).tolist()\n",
    "\n",
    "# The rest stays the same\n",
    "query_texts = df_query_train['tweet_text'].tolist()\n",
    "true_uids = df_query_train['cord_uid'].tolist()\n",
    "collection_uids = df_collection['cord_uid'].tolist()\n",
    "\n",
    "# Tokenize and build index\n",
    "tokenized_corpus = [doc.split() for doc in collection_texts]\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "positive_pairs = []\n",
    "negative_pairs = []\n",
    "\n",
    "for qtext, true_uid in zip(query_texts, true_uids):\n",
    "    # Positive\n",
    "    try:\n",
    "        pos_idx = collection_uids.index(true_uid)\n",
    "        pos_doc = collection_texts[pos_idx]\n",
    "        positive_pairs.append((qtext, pos_doc, 1))\n",
    "    except ValueError:\n",
    "        continue\n",
    "\n",
    "    # Hard Negatives from BM25\n",
    "    scores = bm25.get_scores(qtext.split())\n",
    "    top_indices = np.argsort(scores)[::-1]\n",
    "    negs = 0\n",
    "    for idx in top_indices:\n",
    "        if collection_uids[idx] != true_uid:\n",
    "            negative_pairs.append((qtext, collection_texts[idx], 0))\n",
    "            negs += 1\n",
    "        if negs == 1:\n",
    "            break\n",
    "\n",
    "df_query_title_abstract_triples = pd.DataFrame(positive_pairs + negative_pairs, columns=[\"query\", \"document\", \"label\"])\n",
    "df_query_title_abstract_triples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbdf6e6f-e2b4-49a7-b24d-9b6d8a1b8e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_query_title_abstract_triples.to_csv(\"df_query_title_abstract_triples.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b4a2df-b2c2-462b-8ae9-8c06a46d60c2",
   "metadata": {},
   "source": [
    "### Prepare the training data\n",
    "Convert your df_train_pairs.csv into InputExamples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8cef491a-023f-4f4a-b106-bbd2a943a7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_def.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_avx2.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_core.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_intel_lp64.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_intel_thread.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "/bin/bash: -c: line 1: syntax error near unexpected token `os.getpid'\n",
      "/bin/bash: -c: line 1: `os.kill(os.getpid(), 9)'\n"
     ]
    }
   ],
   "source": [
    "!os.kill(os.getpid(), 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41bb9567-ae22-4f8f-a662-e6cb17ecff71",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"df_query_title_abstract_triples.csv\")\n",
    "#df = df[df['label'] != 0]\n",
    "#df = df.sample(frac=0.5)\n",
    "\n",
    "train_examples = [\n",
    "    InputExample(texts=[row['query'], row['document']], label=float(row['label']))\n",
    "    for _, row in df.iterrows()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2df0a378-a50c-427a-924b-917c0c7ecd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('mixedbread-ai/mxbai-embed-large-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dedf8418-b894-40a6-acfe-a04aca9f2831",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=8)\n",
    "train_loss = losses.CosineSimilarityLoss(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7e19de9-feda-4bbe-9dd0-2294ff2eb360",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_def.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_avx2.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_core.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_intel_lp64.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_intel_thread.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_def.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_avx2.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_core.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_intel_lp64.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "ERROR: ld.so: object '/opt/conda/lib/libmkl_intel_thread.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\n",
      "2025-05-29 11:07:25.741920: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-05-29 11:07:25.764245: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-05-29 11:07:25.770740: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-29 11:07:25.793493: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-29 11:07:26.985910: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9642' max='9642' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9642/9642 2:07:09, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.191800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.171700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.165100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.162200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.159100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.154100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.124800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.113900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.113300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.115300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.114700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.112200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.106200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.073000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.077600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.074900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.074800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.074900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.075600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    epochs=3,\n",
    "    warmup_steps=100,\n",
    "    output_path='mxbai-finetuned-tweet2paper',\n",
    "    use_amp=True  # ← THIS helps reduce memory usage\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da224805-9b7c-486a-84e9-3bd2c304ebe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('mxbai-finetuned-tweet2paper', device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4241c0f-af96-4850-916a-3c126956681f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d465e3ea05064f01b64fe85d9c509030",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/242 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "051682f7642e412e93d7db56930cfc2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/402 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09589b4baa2044bb88a263751e3fac57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For document embeddings\n",
    "text_batch = [title + model.tokenizer.sep_token + abstract for title, abstract in zip(df_collection['title'], df_collection['abstract'])]\n",
    "emb_collection = model.encode(text_batch, batch_size=32, show_progress_bar=True)\n",
    "\n",
    "# For query training embeddings\n",
    "text_finetuned_query_train = df_query_train['tweet_text'].tolist()\n",
    "emb_finetuned_query_train = model.encode(text_finetuned_query_train, batch_size=32, show_progress_bar=True)\n",
    "\n",
    "# For query dev embeddings\n",
    "text_finetuned_query_dev = df_query_dev['tweet_text'].tolist()\n",
    "emb_finetuned_query_dev = model.encode(text_finetuned_query_dev, batch_size=32, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "878bf687-7351-4c0b-937e-aa665ec84ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_query_train['topk'] = get_top_k_cords(emb_finetuned_query_train, emb_collection, df_collection)\n",
    "df_query_dev['topk'] = get_top_k_cords(emb_finetuned_query_dev, emb_collection, df_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ada74679-efbf-4abf-9fe8-b63396d5f359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results on the train set: {1: 0.6068622111569283, 5: 0.6703895329235716, 10: 0.6771817523918201}\n",
      "Results on the dev set: {1: 0.3557142857142857, 5: 0.4024523809523809, 10: 0.4095833333333334}\n"
     ]
    }
   ],
   "source": [
    "results_finetuned_train = get_performance_mrr(df_query_train, 'cord_uid', 'topk')\n",
    "results_finetuned_dev = get_performance_mrr(df_query_dev, 'cord_uid', 'topk')\n",
    "\n",
    "print(f\"Results on the train set: {results_finetuned_train}\")\n",
    "print(f\"Results on the dev set: {results_finetuned_dev}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da75bc1e-2280-42df-8326-74ef3ab8a1d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
