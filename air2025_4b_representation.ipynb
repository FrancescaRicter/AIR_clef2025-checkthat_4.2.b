{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9df5bb7a-3052-41c5-9c76-04a25ab63121",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37abb6ec-cf98-49c8-b36b-398f9f70f8bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH_COLLECTION_DATA = 'https://gitlab.com/checkthat_lab/clef2025-checkthat-lab/-/raw/701a0a217286555445870e1005d637ff587c5cee/task4/subtask_4b/subtask4b_collection_data.pkl'\n",
    "PATH_QUERY_TRAIN_DATA = 'https://gitlab.com/checkthat_lab/clef2025-checkthat-lab/-/raw/main/task4/subtask_4b/subtask4b_query_tweets_train.tsv?inline=false'\n",
    "PATH_QUERY_DEV_DATA = 'https://gitlab.com/checkthat_lab/clef2025-checkthat-lab/-/raw/main/task4/subtask_4b/subtask4b_query_tweets_dev.tsv?inline=false'\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "df_collection = pd.read_pickle(PATH_COLLECTION_DATA)\n",
    "df_query_train = pd.read_csv(PATH_QUERY_TRAIN_DATA, sep = '\\t')\n",
    "df_query_dev = pd.read_csv(PATH_QUERY_DEV_DATA, sep = '\\t')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a102e75d-3dc9-4b3e-9b6e-f8d523b9aa73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Encoding: 100%|██████████| 242/242 [03:50<00:00,  1.05it/s]\n"
     ]
    }
   ],
   "source": [
    "class EmbeddingWrapper:\n",
    "    def __init__(self, text_list, model_name, device):\n",
    "        self.text_list = text_list\n",
    "        self.model_name = model_name\n",
    "        self.device = device\n",
    "    def calculate_embeddings(self, batch_size = 32):\n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        model = AutoModel.from_pretrained(self.model_name).to(self.device)\n",
    "        model.eval()\n",
    "    \n",
    "        embeddings = []\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            for i in tqdm(range(0, len(self.text_list), batch_size), desc=\"Encoding\"):\n",
    "                batch_texts = self.text_list[i:i+batch_size]\n",
    "                inputs = tokenizer(batch_texts, padding=True, truncation=True, return_tensors=\"pt\",\n",
    "                                   return_token_type_ids=False, max_length=512)\n",
    "                inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "                outputs = model(**inputs)\n",
    "                batch_embeddings = outputs.last_hidden_state[:, 0, :] \n",
    "                embeddings.append(batch_embeddings.cpu())\n",
    "    \n",
    "        self.embeddings = torch.cat(embeddings, dim=0)\n",
    "        return self\n",
    "tokenizer =  AutoTokenizer.from_pretrained('allenai/specter2_base')       \n",
    "text_batch = [title + tokenizer.sep_token + abstract for title, abstract in zip(df_collection['title'], df_collection['abstract'])]\n",
    "emb_collection = EmbeddingWrapper(text_batch,model_name='allenai/specter2_base', device=device).calculate_embeddings(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66d352f7-8da6-420f-a768-b189d2b38754",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Encoding: 100%|██████████| 402/402 [00:53<00:00,  7.50it/s]\n"
     ]
    }
   ],
   "source": [
    "text_query_train = df_query_train['tweet_text'].to_list()\n",
    "emb_query_train = EmbeddingWrapper(text_query_train,model_name='allenai/specter2_base', device=device).calculate_embeddings(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec8738fb-be84-44ef-a4c8-a38d9235cea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_performance_mrr(data, col_gold, col_pred, list_k = [1, 5, 10]):\n",
    "    d_performance = {}\n",
    "    for k in list_k:\n",
    "        data[\"in_topx\"] = data.apply(lambda x: (1/([i for i in x[col_pred][:k]].index(x[col_gold]) + 1) if x[col_gold] in [i for i in x[col_pred][:k]] else 0), axis=1)\n",
    "        #performances.append(data[\"in_topx\"].mean())\n",
    "        d_performance[k] = data[\"in_topx\"].mean()\n",
    "    return d_performance\n",
    "    \n",
    "def get_top_k_cords(emb_query, emb_collection, df_collection, k=30):\n",
    "    # compute cosine similarity matrix (for each query consine similarity for each document)\n",
    "    cos_sim_matrix = cosine_similarity(emb_query, emb_collection)\n",
    "    # For each query, get the indices of the top-k documents\n",
    "    top_k_indices = np.argsort(-cos_sim_matrix, axis=1)[:, :k]  # shape: (num_queries, k)\n",
    "\n",
    "    top_k_cord_uids = df_collection.iloc[top_k_indices.flatten()]['cord_uid'].values.reshape(top_k_indices.shape) # shape: (num_queries_topcords)\n",
    "    return top_k_cord_uids.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "886954dc-f82c-41b3-ba68-1fa016a0ae1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_query_train['topk_specter_v1'] = get_top_k_cords(emb_query_train.embeddings, emb_collection.embeddings, df_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e18b9705-0bfe-4fa1-8cb8-9e40409ef52d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0.403096553333852, 5: 0.4702948727923443, 10: 0.47891286575056896}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_performance_mrr(df_query_train, 'cord_uid', 'topk_specter_v1')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "air2025_py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
